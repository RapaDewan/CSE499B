{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2508632,"sourceType":"datasetVersion","datasetId":1519260}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ“Œ Cell 1: List All PDF Files in the Dataset","metadata":{}},{"cell_type":"code","source":"import os\n\n# Walk through the Kaggle input directory and list only PDF files.\npdf_files = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if filename.lower().endswith(\".pdf\"):\n            file_path = os.path.join(dirname, filename)\n            pdf_files.append(file_path)\n            #print(file_path)\n\nprint(f\"\\nTotal PDF files found: {len(pdf_files)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T18:33:45.634767Z","iopub.execute_input":"2025-03-12T18:33:45.635068Z","iopub.status.idle":"2025-03-12T18:33:48.787970Z","shell.execute_reply.started":"2025-03-12T18:33:45.635046Z","shell.execute_reply":"2025-03-12T18:33:48.786919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install pdfplumber","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T18:33:48.789294Z","iopub.execute_input":"2025-03-12T18:33:48.789649Z","iopub.status.idle":"2025-03-12T18:33:55.585246Z","shell.execute_reply.started":"2025-03-12T18:33:48.789603Z","shell.execute_reply":"2025-03-12T18:33:55.583976Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ“Œ Cell 2: Improved Text Extraction from PDFs","metadata":{}},{"cell_type":"code","source":"import pdfplumber\n\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"\n    Extracts text from a PDF file using pdfplumber.\n    \n    Parameters:\n        pdf_path (str): Path to the PDF file.\n    \n    Returns:\n        text (str): Extracted text.\n    \"\"\"\n    text = \"\"\n    try:\n        with pdfplumber.open(pdf_path) as pdf:\n            for page in pdf.pages:\n                page_text = page.extract_text() or \"\"\n                text += page_text + \"\\n\"\n    except Exception as e:\n        print(f\"Error extracting text from {pdf_path}: {e}\")\n    return text.strip()\n\n# Test the function on the first PDF file if needed:\n# print(extract_text_from_pdf(pdf_files[0])[:500])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T18:34:11.839059Z","iopub.execute_input":"2025-03-12T18:34:11.839415Z","iopub.status.idle":"2025-03-12T18:34:12.070801Z","shell.execute_reply.started":"2025-03-12T18:34:11.839385Z","shell.execute_reply":"2025-03-12T18:34:12.070027Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ“Œ Cell 3: Load and Process All PDF Resumes","metadata":{}},{"cell_type":"code","source":"def read_resumes_from_files(file_list):\n    \"\"\"\n    Reads all PDF resumes from a list of file paths and extracts text.\n    \n    Parameters:\n        file_list (list): List of PDF file paths.\n    \n    Returns:\n        resumes (list): List of extracted resume texts.\n    \"\"\"\n    resumes = []\n    for file_path in file_list:\n        text = extract_text_from_pdf(file_path)\n        if text:  # Only add if text extraction was successful\n            resumes.append(text)\n    print(f\"Total resumes processed: {len(resumes)}\")\n    return resumes\n\n# Load all resumes from the collected PDF file paths.\nall_resumes = read_resumes_from_files(pdf_files)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T18:38:27.175844Z","iopub.execute_input":"2025-03-12T18:38:27.176236Z","iopub.status.idle":"2025-03-12T19:10:05.069642Z","shell.execute_reply.started":"2025-03-12T18:38:27.176201Z","shell.execute_reply":"2025-03-12T19:10:05.068371Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 4: Enhanced Preprocessing with spaCy (Lemmatization & Stopword Removal)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport spacy\n\n# Load spaCy's English model (make sure this model is available on Kaggle)\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef preprocess_text_spacy(text):\n    \"\"\"\n    Preprocesses resume text by removing extra spaces, lowercasing, lemmatizing, \n    and removing stopwords and punctuation using spaCy.\n    \n    Parameters:\n        text (str): Original text.\n    \n    Returns:\n        processed_text (str): Preprocessed text.\n    \"\"\"\n    # Clean up spaces/newlines\n    text = re.sub(r'\\s+', ' ', text)\n    doc = nlp(text)\n    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n    return \" \".join(tokens)\n\n# Preprocess all resumes.\nprocessed_resumes = [preprocess_text_spacy(resume) for resume in all_resumes]\nprint(\"âœ… Text preprocessing complete with spaCy!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:10:05.071306Z","iopub.execute_input":"2025-03-12T19:10:05.071692Z","iopub.status.idle":"2025-03-12T19:15:52.281709Z","execution_failed":"2025-03-12T20:06:30.858Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 5: Compute Sentenceâ€‘BERT Embeddings (Replace Word2Vec/TFâ€‘IDF)","metadata":{}},{"cell_type":"code","source":"!pip install -q sentence-transformers\n\nfrom sentence_transformers import SentenceTransformer\nimport torch\n\n# Load a pre-trained Sentence-BERT model.\nsbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Compute embeddings for each processed resume.\nresume_embeddings = sbert_model.encode(processed_resumes, convert_to_tensor=True)\nprint(\"âœ… Sentence-BERT embeddings computed!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:15:52.289449Z","iopub.execute_input":"2025-03-12T19:15:52.289704Z","iopub.status.idle":"2025-03-12T19:18:56.311336Z","execution_failed":"2025-03-12T20:06:30.858Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ“Œ Cell 6: Bias Mitigation â€“ Remove Demographic Indicators","metadata":{}},{"cell_type":"code","source":"def remove_demographic_indicators(text):\n    \"\"\"\n    Removes demographic indicators (e.g., names, locations) using spaCy's NER.\n    \n    Parameters:\n        text (str): Input text.\n    \n    Returns:\n        cleaned_text (str): Text with demographic entities removed.\n    \"\"\"\n    doc = nlp(text)\n    tokens = [token.text for token in doc if token.ent_type_ not in [\"PERSON\", \"GPE\"]]\n    return \" \".join(tokens)\n\n# Apply bias mitigation on the processed resumes.\ndebiased_resumes = [remove_demographic_indicators(text) for text in processed_resumes]\nprint(\"âœ… Bias mitigation applied on resume texts!\")\n\n# (Optional) Recompute embeddings on debiased resumes for ranking:\ndebiased_embeddings = sbert_model.encode(debiased_resumes, convert_to_tensor=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:18:56.314898Z","iopub.execute_input":"2025-03-12T19:18:56.315179Z","iopub.status.idle":"2025-03-12T19:25:31.452484Z","execution_failed":"2025-03-12T20:06:30.859Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ“Œ Cell 7: Enhanced Feedback Generation Ranking with Sentenceâ€‘BERT","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\n# Define your job description for ranking.\njob_description = \"We are seeking a skilled designer with strong experience in graphic design, UI/UX, and creative problem solving.\"\n\n# Compute the job description embedding.\njob_embedding = sbert_model.encode(job_description, convert_to_tensor=True)\n\n# Convert tensors to NumPy arrays (if needed for cosine similarity)\njob_embedding_np = job_embedding.cpu().numpy()\ndebiased_embeddings_np = debiased_embeddings.cpu().numpy()\n\n# Compute cosine similarity between the job description and each resume.\nsimilarities = cosine_similarity([job_embedding_np], debiased_embeddings_np)[0]\n\n# Get ranked indices (highest similarity first).\nranked_indices = np.argsort(similarities)[::-1]\n\nprint(\"Ranking complete. Top 5 similarity scores:\")\nfor i in range(min(5, len(similarities))):\n    print(f\"Rank {i+1}: Resume Index {ranked_indices[i]} with similarity {similarities[ranked_indices[i]]:.4f}\")\n\n# Feedback functions (you can later extend these with more advanced interpretable methods).\ndef recruiter_feedback(resume_text):\n    if len(resume_text.split()) < 50:\n        return \"This resume may lack sufficient details.\"\n    elif \"experience\" not in resume_text:\n        return \"Consider looking for resumes with clear experience details.\"\n    else:\n        return \"Resume appears well-detailed.\"\n\ndef job_seeker_feedback(resume_text):\n    missing_keywords = []\n    essential_keywords = [\"experience\", \"skills\", \"education\", \"projects\"]\n    for keyword in essential_keywords:\n        if keyword not in resume_text:\n            missing_keywords.append(keyword)\n    if missing_keywords:\n        return f\"Consider adding: {', '.join(missing_keywords)}.\"\n    else:\n        return \"Your resume appears comprehensive!\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:25:31.493903Z","iopub.execute_input":"2025-03-12T19:25:31.494350Z","iopub.status.idle":"2025-03-12T19:25:31.551513Z","execution_failed":"2025-03-12T20:06:30.859Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ“Œ Cell 8: Main Execution â€“ Process, Generate Feedback, and Save Results","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nresults = []\n# Use the ranking from the cosine similarity computed on debiased embeddings.\nfor rank, idx in enumerate(ranked_indices, start=1):\n    resume_text = debiased_resumes[idx]\n    rec_feedback = recruiter_feedback(resume_text)\n    cand_feedback = job_seeker_feedback(resume_text)\n    \n    results.append({\n        \"Rank\": rank,\n        \"Resume Index\": idx + 1,  # converting 0-index to 1-index for display\n        \"Similarity Score\": similarities[idx],\n        \"Recruiter Feedback\": rec_feedback,\n        \"Job Seeker Feedback\": cand_feedback,\n        \"Resume Snippet\": resume_text[:500] + \"...\"\n    })\n\n# Convert the results to a DataFrame.\nresults_df = pd.DataFrame(results)\n\n# Save the results to a CSV file.\noutput_path = \"/kaggle/working/resume_feedback_results_with_ranking.csv\"\nresults_df.to_csv(output_path, index=False)\nprint(f\"âœ… Results saved to {output_path}\")\n\n# Display the first few rows of the results.\nresults_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:25:31.558024Z","iopub.execute_input":"2025-03-12T19:25:31.558372Z","iopub.status.idle":"2025-03-12T19:25:31.799537Z","execution_failed":"2025-03-12T20:06:30.859Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:25:31.800330Z","iopub.execute_input":"2025-03-12T19:25:31.800580Z","iopub.status.idle":"2025-03-12T19:25:31.811010Z","execution_failed":"2025-03-12T20:06:30.859Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ“Œ Cell 9: View a Specific CV (e.g., Top-Ranked Resume)","metadata":{}},{"cell_type":"code","source":"from IPython.display import IFrame\n\n# To view the top-ranked resume, use the first element from ranked_indices.\ntop_resume_index = ranked_indices[0]\ntop_resume_file = pdf_files[top_resume_index]\n\nprint(f\"Displaying the top-ranked resume from file: {top_resume_file}\")\nIFrame(top_resume_file, width=800, height=600)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:25:31.812374Z","iopub.execute_input":"2025-03-12T19:25:31.812789Z","iopub.status.idle":"2025-03-12T19:25:31.819652Z","execution_failed":"2025-03-12T20:06:30.860Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!lscpu\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:25:31.820452Z","iopub.execute_input":"2025-03-12T19:25:31.820772Z","iopub.status.idle":"2025-03-12T19:25:31.994285Z","execution_failed":"2025-03-12T20:06:30.860Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}